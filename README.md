# pipeline_apply
main motive is to "how we apply pipeline"(using titanic datasets)


## Titanic Dataset Analysis and Model Building with Scikit-Learn Pipelines
### Overview
This repository demonstrates how to apply Scikit-Learn pipelines for preprocessing and building a machine learning model using the Titanic dataset. The primary focus is on showcasing the use of pipelines to streamline the data processing and model training steps.
### Dataset
The Titanic dataset is used for this project, containing information about passengers such as age, sex, class, and survival status.
Dataset Source: Kaggle - Titanic: Machine Learning from Disaster

### Project Structure
The project structure is organized as follows:<br>

How to Apply Pipeline<br>
### 1. Install Dependencies<br>
Make sure you have the required Python packages installed. You can install them using the following command:
#### pip install -r requirements.txt<br>

### 2. Run the Jupyter Notebook<br>
Open and run the notebooks/Titanic_Pipeline.ipynb notebook, which provides a step-by-step guide on applying pipelines for data preprocessing and model building.

### 3. Explore the Notebook<br>
The notebook covers the following key steps:

### Data Loading: Read the Titanic dataset.<br>
Data Preprocessing: Handle missing values, encode categorical features, and scale numerical features using a Scikit-Learn pipeline.
Model Building: Create a pipeline that combines preprocessing and model training using a DecisionTreeClassifier.
Model Evaluation: Evaluate the model's performance using cross-validation.


### 4. Customize and Extend<br>
Feel free to customize the notebook or extend the functionality based on your specific requirements. The provided code serves as a starting point for implementing pipelines in your machine learning projects.
